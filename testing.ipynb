{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "import gensim\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from unidecode import unidecode\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "nltk.download('punkt')\n",
    "from google.colab import drive\n",
    "drive.mount('data')\n",
    "\n",
    "# def clean_questions(questions):\n",
    "#     cleaned_questions = []\n",
    "#     for question in questions:\n",
    "#         question = question.lower()\n",
    "#         question = unidecode(question)\n",
    "#         cleaned_question = re.sub(r\"[^\\w\\s]\", \"\", question) \n",
    "#         cleaned_questions.append(cleaned_question)\n",
    "#     return cleaned_questions\n",
    "\n",
    "\n",
    "# training_loc = sys.argv[1]\n",
    "# testing_loc = sys.argv[2]\n",
    "\n",
    "\n",
    "questions_train = []\n",
    "tables_train = []\n",
    "actual_col_train = []\n",
    "label_cols_train = []\n",
    "label_rows_train = []\n",
    "with open('data/MyDrive/A2_train.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parsed_data = json.loads(line)\n",
    "        questions_train.append(parsed_data['question'])\n",
    "        tables_train.append(parsed_data['table'])\n",
    "        label_cols_train.append(parsed_data['label_col'][0])\n",
    "        actual_col_train.append(list(parsed_data['table']['cols']))\n",
    "        label_rows_train.append(list(parsed_data['label_row']))\n",
    "\n",
    "# questions_train = clean_questions(questions_train)\n",
    "\n",
    "print('Number of questions:', len(questions_train))\n",
    "print('Number of tables:', len(tables_train))\n",
    "print('Number of label columns:', len(label_cols_train))\n",
    "print('Number of actual columns:', len(actual_col_train))\n",
    "print('Number of label rows:', len(label_rows_train))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "questions_test = []\n",
    "tables_test = []\n",
    "actual_col_test = []\n",
    "label_cols_test = []\n",
    "label_rows_test = []\n",
    "qid_test = []\n",
    "\n",
    "with open('data/MyDrive/A2_val.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parsed_data = json.loads(line)\n",
    "        questions_test.append(parsed_data['question'])\n",
    "        tables_test.append(parsed_data['table'])\n",
    "        label_cols_test.append(parsed_data['label_col'][0])\n",
    "        actual_col_test.append(list(parsed_data['table']['cols']))\n",
    "        qid_test.append(parsed_data['qid'])\n",
    "        label_rows_test.append(list(parsed_data['label_row']))\n",
    "\n",
    "# questions_test = clean_questions(questions_test)\n",
    "\n",
    "print('Number of questions:', len(questions_test))\n",
    "print('Number of tables:', len(tables_test))\n",
    "print('Number of label columns:', len(label_cols_test))\n",
    "print('Number of actual columns:', len(actual_col_test))\n",
    "print('Number of qids is ', len(qid_test))\n",
    "print('Number of label rows:', len(label_rows_test))\n",
    "\n",
    "\n",
    "model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "\n",
    "# predictions_train_row = []\n",
    "# for i in range(len(questions_train)):\n",
    "#     question = unidecode(questions_train[i]).lower()\n",
    "#     table = tables_train[i]\n",
    "#     actual_col = actual_col_train[i]\n",
    "#     label_col = label_cols_train[i][0]\n",
    "#     label_row = label_rows_train[i]\n",
    "#     predicted_row = []\n",
    "#     score_of_row = []\n",
    "#     for j in range(len(table['rows'])):\n",
    "#         row = table['rows'][j]\n",
    "#         score = 0.0\n",
    "#         for k in range(len(row)):\n",
    "#             cell = unidecode(row[k]).lower()\n",
    "#             that_col = unidecode(actual_col[k]).lower()\n",
    "#             if cell in question:\n",
    "#                 if that_col in question:\n",
    "#                     distance = abs(question.index(cell) - question.index(that_col))\n",
    "#                     score += (43.0)*(len(cell)*len(cell))/(distance+1)\n",
    "#                 else:\n",
    "#                     score += len(cell)*len(cell)\n",
    "#         score_of_row.append(score)\n",
    "    \n",
    "#     max_score = max(score_of_row)\n",
    "#     for i in range(len(score_of_row)):\n",
    "#         if score_of_row[i] == max_score:\n",
    "#             predicted_row.append(i)\n",
    "#     predictions_train_row.append(predicted_row)\n",
    "\n",
    "\n",
    "\n",
    "max_len_question = 60\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pos_em = torch.zeros(max_len_question, embedding_dim)\n",
    "        division = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        position = torch.arange(0, max_len_question, dtype=torch.float).unsqueeze(1)\n",
    "        pos_em[:, 0::2] = torch.sin(position * division)\n",
    "        pos_em[:, 1::2] = torch.cos(position * division)\n",
    "        self.register_buffer('pos_em', pos_em)\n",
    "\n",
    "    def forward(self, temp):\n",
    "        return temp+self.pos_em\n",
    "    \n",
    "embedding_dimension = 100\n",
    "hidden_dimension = 250\n",
    "num_layers = 2\n",
    "num_heads = 1\n",
    "dropout = 0.03\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, num_heads, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout,batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        self.pos_embed = PositionalEmbedding(embedding_dim)\n",
    "\n",
    "    def forward(self, text_vectors, column):\n",
    "        input_embedding = self.pos_embed(text_vectors)\n",
    "        contextual_embedding = self.transformer_encoder(input_embedding)\n",
    "        question_embedding  = torch.sum(contextual_embedding,dim = 1)\n",
    "        mat_mul = torch.nn.functional.normalize(column,dim = 2) * torch.nn.functional.normalize(question_embedding.unsqueeze(1), dim = 2)\n",
    "        dot_prod = torch.sum(mat_mul, dim=2)\n",
    "        return dot_prod\n",
    "\n",
    "\n",
    "def word2vec_questions(questions):\n",
    "    final_word2vec = []\n",
    "    for i in range(len(questions)):\n",
    "        ques = questions[i]\n",
    "        ques = unidecode(ques)\n",
    "        ques_tokens = nltk.word_tokenize(ques.lower())\n",
    "        word2vec = []\n",
    "        for j in range(min(60,len(ques_tokens))):\n",
    "            token = ques_tokens[j]\n",
    "            try:\n",
    "                word2vec.append(torch.tensor(model[token]))\n",
    "            except:\n",
    "                pass\n",
    "        while len(word2vec) < max_len_question:\n",
    "            word2vec.append(torch.zeros(100))\n",
    "        word2vec = torch.stack(word2vec, dim=0)\n",
    "        final_word2vec.append(word2vec)\n",
    "    return final_word2vec\n",
    "\n",
    "def one_hot_label(actual_col, label_col):\n",
    "    one_hot = torch.zeros((len(actual_col), 64), dtype=float)\n",
    "    for i in range(len(actual_col)):\n",
    "        for j in range(len(actual_col[i])):\n",
    "            if actual_col[i][j] == label_col[i]:\n",
    "                one_hot[i][j] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def column_embed(actual_col):\n",
    "    final_embed = []\n",
    "    for i in range(len(actual_col)):\n",
    "        col = actual_col[i]\n",
    "        word_embed = []\n",
    "        for j in range(len(col)):\n",
    "            temp = unidecode(col[j])\n",
    "            tokens = nltk.word_tokenize(temp.lower())\n",
    "            within_word_embed = []\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    within_word_embed.append(torch.tensor(model[token]))\n",
    "                except:\n",
    "                    within_word_embed.append(torch.zeros(100))\n",
    "            within_word_embed = torch.sum(torch.stack(within_word_embed, dim=0), dim = 0)\n",
    "            word_embed.append(within_word_embed)\n",
    "        while len(word_embed) < 64:\n",
    "            word_embed.append(torch.zeros(100))\n",
    "        final_embed.append(torch.stack(word_embed, dim=0))\n",
    "    return final_embed\n",
    "\n",
    "\n",
    "questions_vectors_train = word2vec_questions(questions_train)\n",
    "questions_vectors_test = word2vec_questions(questions_test)\n",
    "\n",
    "one_hot_label_train = one_hot_label(actual_col_train, label_cols_train)\n",
    "one_hot_label_test = one_hot_label(actual_col_test, label_cols_test)\n",
    "\n",
    "col_embeddings_train = column_embed(actual_col_train)\n",
    "col_embeddings_test = column_embed(actual_col_test)\n",
    "\n",
    "classifier = Classifier(embedding_dimension, hidden_dimension, num_layers, num_heads, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.005)\n",
    "\n",
    "classifier.train()\n",
    "\n",
    "batched_data_train = []\n",
    "for i in range(len(questions_vectors_train)):\n",
    "    temp_list = []\n",
    "    temp_list.append(questions_vectors_train[i])\n",
    "    temp_list.append(col_embeddings_train[i])\n",
    "    temp_list.append(one_hot_label_train[i])\n",
    "    batched_data_train.append(temp_list)\n",
    "\n",
    "batched_data_test = []\n",
    "for i in range(len(questions_vectors_test)):\n",
    "    temp_list = []\n",
    "    temp_list.append(questions_vectors_test[i])\n",
    "    temp_list.append(col_embeddings_test[i])\n",
    "    temp_list.append(one_hot_label_test[i])\n",
    "    batched_data_test.append(temp_list)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "random.shuffle(batched_data_train)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(300):\n",
    "    acc_train = 0\n",
    "    # random.shuffle(batched_data)\n",
    "    k=0\n",
    "    for i in range(0, len(batched_data_train), 5000):\n",
    "        k+=1\n",
    "        batch = batched_data_train[i:i+5000]\n",
    "        # print(len(batch))\n",
    "        batch_ques_train = []\n",
    "        batch_col_train = []\n",
    "        batch_lab_train = []\n",
    "        for bat in batch:\n",
    "            batch_ques_train.append(bat[0])\n",
    "            batch_col_train.append(bat[1])\n",
    "            batch_lab_train.append(bat[2])\n",
    "        batch_ques_train = torch.stack(batch_ques_train, dim=0).to(device)\n",
    "        batch_col_train = torch.stack(batch_col_train, dim=0).to(device)\n",
    "        batch_lab_train = torch.stack(batch_lab_train, dim=0).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_batch_train = classifier(batch_ques_train, batch_col_train)\n",
    "        loss = criterion(outputs_batch_train, batch_lab_train)\n",
    "        acc_train += (outputs_batch_train.argmax(dim=1) == batch_lab_train.argmax(dim=1)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if k == 5:\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            acc_train= 0\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        acc_test = 0\n",
    "        for i in range(0, len(batched_data_test), 1000):\n",
    "            batch = batched_data_test[i:i+1000]\n",
    "            batch_ques_test = []\n",
    "            batch_col_test = []\n",
    "            batch_lab_test = []\n",
    "            for bat in batch:\n",
    "                batch_ques_test.append(bat[0])\n",
    "                batch_col_test.append(bat[1])\n",
    "                batch_lab_test.append(bat[2])\n",
    "            batch_ques_test = torch.stack(batch_ques_test, dim=0).to(device)\n",
    "            batch_col_test = torch.stack(batch_col_test, dim=0).to(device)\n",
    "            batch_lab_test = torch.stack(batch_lab_test, dim=0).to(device)\n",
    "            outputs_batch_test = classifier(batch_ques_test, batch_col_test)\n",
    "            acc_test += (outputs_batch_test.argmax(dim=1) == batch_lab_test.argmax(dim=1)).sum().item()\n",
    "        print(f'Accuracy of the network on the test data: {acc_test/len(questions_test)}')\n",
    "\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# torch.save(classifier.state_dict(), './model')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
