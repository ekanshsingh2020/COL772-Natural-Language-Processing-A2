{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "import gensim\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# from google.colab import drive\n",
    "# drive.mount('data')\n",
    "\n",
    "\n",
    "questions_train = []\n",
    "tables_train = []\n",
    "actual_col_train = []\n",
    "label_cols_train = []\n",
    "with open('data/MyDrive/A2_train.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parsed_data = json.loads(line)\n",
    "        questions_train.append(parsed_data['question'])\n",
    "        tables_train.append(parsed_data['table'])\n",
    "        label_cols_train.append(parsed_data['label_col'][0])\n",
    "        actual_col_train.append(list(parsed_data['table']['cols']))\n",
    "\n",
    "print('Number of questions:', len(questions_train))\n",
    "print('Number of tables:', len(tables_train))\n",
    "print('Number of label columns:', len(label_cols_train))\n",
    "print('Number of actual columns:', len(actual_col_train))\n",
    "\n",
    "questions_test = []\n",
    "tables_test = []\n",
    "actual_col_test = []\n",
    "label_cols_test = []\n",
    "qid_test = []\n",
    "with open('data/MyDrive/A2_val.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parsed_data = json.loads(line)\n",
    "        questions_test.append(parsed_data['question'])\n",
    "        tables_test.append(parsed_data['table'])\n",
    "        label_cols_test.append(parsed_data['label_col'][0])\n",
    "        actual_col_test.append(list(parsed_data['table']['cols']))\n",
    "        qid_test.append(parsed_data['qid'])\n",
    "\n",
    "print('Number of questions:', len(questions_test))\n",
    "print('Number of tables:', len(tables_test))\n",
    "print('Number of label columns:', len(label_cols_test))\n",
    "print('Number of actual columns:', len(actual_col_test))\n",
    "print('Number of qids is ', len(qid_test))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "embedding_dimension = 100\n",
    "hidden_dimension = 256\n",
    "num_layers = 2\n",
    "num_heads = 1\n",
    "dropout = 0.02\n",
    "\n",
    "max_len_question = 60\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pos_em = torch.zeros(max_len_question, embedding_dim)\n",
    "        division = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        position = torch.arange(0, max_len_question, dtype=torch.float).unsqueeze(1)\n",
    "        pos_em[:, 0::2] = torch.sin(position * division)\n",
    "        pos_em[:, 1::2] = torch.cos(position * division)\n",
    "        self.register_buffer('pos_em', pos_em)\n",
    "\n",
    "    def forward(self, temp):\n",
    "        return temp+self.pos_em\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, num_heads, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout,batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        self.pos_embed = PositionalEmbedding(embedding_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text_vectors, column):\n",
    "        input_embedding = self.pos_embed(text_vectors)\n",
    "        contextual_embedding = self.transformer_encoder(input_embedding)\n",
    "        question_embedding  = torch.sum(contextual_embedding,dim = 1)\n",
    "        mat_mul = torch.nn.functional.normalize(column,dim = 2) * torch.nn.functional.normalize(question_embedding.unsqueeze(1), dim = 2)\n",
    "        dot_prod = torch.sum(mat_mul, dim=2)\n",
    "        return dot_prod\n",
    "\n",
    "\n",
    "def word2vec_questions(questions):\n",
    "    final_word2vec = []\n",
    "    for i in range(len(questions)):\n",
    "        ques = questions[i]\n",
    "        ques = unidecode(ques)\n",
    "        ques_tokens = nltk.word_tokenize(ques.lower())\n",
    "        word2vec = []\n",
    "\n",
    "        for token in ques_tokens:\n",
    "            try:\n",
    "                word2vec.append(torch.tensor(model[token]))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        while len(word2vec) < max_len_question:\n",
    "            word2vec.append(torch.zeros(100))\n",
    "        \n",
    "        word2vec = torch.stack(word2vec, dim=0)\n",
    "        final_word2vec.append(word2vec)\n",
    "    return final_word2vec\n",
    "\n",
    "def one_hot_label(actual_col, label_col):\n",
    "    one_hot = torch.zeros((len(actual_col), 64), dtype=float)\n",
    "    for i in range(len(actual_col)):\n",
    "        for j in range(len(actual_col[i])):\n",
    "            if actual_col[i][j] == label_col[i]:\n",
    "                one_hot[i][j] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def column_embed(actual_col):\n",
    "    final_embed = []\n",
    "    for i in range(len(actual_col)):\n",
    "        col = actual_col[i]\n",
    "        word_embed = []\n",
    "        for j in range(len(col)):\n",
    "            temp = unidecode(col[j])\n",
    "            tokens = nltk.word_tokenize(temp.lower())\n",
    "            within_word_embed = []\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    within_word_embed.append(torch.tensor(model[token]))\n",
    "                except:\n",
    "                    within_word_embed.append(torch.zeros(100))\n",
    "            within_word_embed = torch.sum(torch.stack(within_word_embed, dim=0), dim = 0)\n",
    "            word_embed.append(within_word_embed)\n",
    "        while len(word_embed) < 64:\n",
    "            word_embed.append(torch.zeros(100))\n",
    "        final_embed.append(torch.stack(word_embed, dim=0))\n",
    "    return final_embed\n",
    "\n",
    "\n",
    "questions_vectors_train = word2vec_questions(questions_train)\n",
    "questions_vectors_test = word2vec_questions(questions_test)\n",
    "\n",
    "one_hot_label_train = one_hot_label(actual_col_train, label_cols_train)\n",
    "one_hot_label_test = one_hot_label(actual_col_test, label_cols_test)\n",
    "\n",
    "col_embeddings_train = column_embed(actual_col_train)\n",
    "col_embeddings_test = column_embed(actual_col_test)\n",
    "\n",
    "classifier = Classifier(embedding_dimension, hidden_dimension, num_layers, num_heads, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.005)\n",
    "\n",
    "classifier.train()\n",
    "\n",
    "batched_data = []\n",
    "for i in range(len(questions_vectors_train)):\n",
    "    temp_list = []\n",
    "    temp_list.append(questions_vectors_train[i])\n",
    "    temp_list.append(col_embeddings_train[i])\n",
    "    temp_list.append(one_hot_label_train[i])\n",
    "    batched_data.append(temp_list)\n",
    "\n",
    "val_batched_data = []\n",
    "for i in range(len(questions_vectors_test)):\n",
    "    temp_list = []\n",
    "    temp_list.append(questions_vectors_test[i])\n",
    "    temp_list.append(col_embeddings_test[i])\n",
    "    temp_list.append(one_hot_label_test[i])\n",
    "    val_batched_data.append(temp_list)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "\n",
    "for epoch in range(500):\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    random.shuffle(batched_data)\n",
    "    k=0\n",
    "    for i in range(0, len(batched_data), 5000):\n",
    "        k+=1\n",
    "        batch = batched_data[i:i+5000]\n",
    "        inputs = []\n",
    "        columns = []\n",
    "        labels = []\n",
    "        for bat in batch:\n",
    "            inputs.append(bat[0])\n",
    "            columns.append(bat[1])\n",
    "            labels.append(bat[2])\n",
    "        inputs = torch.stack(inputs, dim=0).to(device)\n",
    "        columns = torch.stack(columns, dim=0).to(device)\n",
    "        labels = torch.stack(labels, dim=0).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(inputs, columns)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if k == 5:\n",
    "            print(f'Epoch {epoch + 1}, batch {i + 1}: loss {running_loss / 5}')\n",
    "            print(f'Accuracy: {accuracy/(25 * 1000)}')\n",
    "            running_loss = 0.0\n",
    "            accuracy = 0\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        val_accuracy = 0\n",
    "        for i in range(0, len(val_batched_data), 1000):\n",
    "            batch = val_batched_data[i:i+1000]\n",
    "            inputs = []\n",
    "            columns = []\n",
    "            labels = []\n",
    "            for bat in batch:\n",
    "                inputs.append(bat[0])\n",
    "                columns.append(bat[1])\n",
    "                labels.append(bat[2])\n",
    "            inputs = torch.stack(inputs, dim=0).to(device)\n",
    "            columns = torch.stack(columns, dim=0).to(device)\n",
    "            labels = torch.stack(labels, dim=0).to(device)\n",
    "            outputs = classifier(inputs, columns)\n",
    "            val_accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "        print(f'Validation accuracy: {val_accuracy/len(questions_test)}')\n",
    "        if val_accuracy/len(questions_test) > 0.9:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
