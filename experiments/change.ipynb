{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import FastText\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "import gensim\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# from google.colab import drive\n",
    "# drive.mount('data')\n",
    "\n",
    "\n",
    "questions_train = []\n",
    "tables_train = []\n",
    "actual_col_train = []\n",
    "label_cols_train = []\n",
    "# Read JSON data from file line by line\n",
    "with open('data/A2_train.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Parse JSON data from each line\n",
    "        parsed_data = json.loads(line)\n",
    "        questions_train.append(parsed_data['question'])\n",
    "        tables_train.append(parsed_data['table'])\n",
    "        label_cols_train.append(parsed_data['label_col'][0])\n",
    "        actual_col_train.append(list(parsed_data['table']['cols']))\n",
    "\n",
    "# questions_train = clean_questions(questions_train)\n",
    "\n",
    "print('Number of questions:', len(questions_train))\n",
    "print('Number of tables:', len(tables_train))\n",
    "print('Number of label columns:', len(label_cols_train))\n",
    "print('Number of actual columns:', len(actual_col_train))\n",
    "\n",
    "questions_test = []\n",
    "tables_test = []\n",
    "actual_col_test = []\n",
    "label_cols_test = []\n",
    "qid_test = []\n",
    "# Read JSON data from file line by line\n",
    "with open('data/A2_val.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Parse JSON data from each line\n",
    "        parsed_data = json.loads(line)\n",
    "        questions_test.append(parsed_data['question'])\n",
    "        tables_test.append(parsed_data['table'])\n",
    "        label_cols_test.append(parsed_data['label_col'][0])\n",
    "        actual_col_test.append(list(parsed_data['table']['cols']))\n",
    "        qid_test.append(parsed_data['qid'])\n",
    "\n",
    "# questions_test = clean_questions(questions_test)\n",
    "\n",
    "print('Number of questions:', len(questions_test))\n",
    "print('Number of tables:', len(tables_test))\n",
    "print('Number of label columns:', len(label_cols_test))\n",
    "print('Number of actual columns:', len(actual_col_test))\n",
    "print('Number of qids is ', len(qid_test))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_len_question = 60\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pos_em = torch.zeros(max_len_question, embedding_dim)\n",
    "        division = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        position = torch.arange(0, max_len_question, dtype=torch.float).unsqueeze(1)\n",
    "        pos_em[:, 0::2] = torch.sin(position * division)\n",
    "        pos_em[:, 1::2] = torch.cos(position * division)\n",
    "        self.register_buffer('pos_em', pos_em)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.pos_em\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, num_heads, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout,batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        self.positional_embedding = PositionalEmbedding(embedding_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text_vectors, column):\n",
    "        pos_embedding = self.positional_embedding()\n",
    "        input_embedding = text_vectors + pos_embedding\n",
    "        contextual_embedding = self.transformer_encoder(input_embedding)\n",
    "        question_embedding  = torch.sum(contextual_embedding,dim = 1)\n",
    "        mat_mul = torch.nn.functional.normalize(column,dim = 2) * torch.nn.functional.normalize(question_embedding.unsqueeze(1), dim = 2)\n",
    "        dot_prod = torch.sum(mat_mul, dim=2)\n",
    "        return dot_prod\n",
    "\n",
    "model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "\n",
    "tokenized_questions = []\n",
    "for question in questions_train:\n",
    "    tokens = nltk.word_tokenize(unidecode(question).lower())\n",
    "\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vectors.append(torch.tensor(model[token]))\n",
    "        except:\n",
    "            pass\n",
    "    # pad to 100 tokens\n",
    "    while len(vectors) < 60:\n",
    "        vectors.append(torch.zeros(100))\n",
    "    # concatenate the vectors to one tensor\n",
    "    vectors = torch.stack(vectors, dim=0)\n",
    "    tokenized_questions.append(vectors)\n",
    "\n",
    "val_tokenized_questions = []\n",
    "for question in questions_test:\n",
    "    tokens = nltk.word_tokenize(unidecode(question).lower())\n",
    "\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vectors.append(torch.tensor(model[token]))\n",
    "        except:\n",
    "            pass\n",
    "    # pad to 100 tokens\n",
    "    while len(vectors) < 60:\n",
    "        vectors.append(torch.zeros(100))\n",
    "    # concatenate the vectors to one tensor\n",
    "    vectors = torch.stack(vectors, dim=0)\n",
    "    val_tokenized_questions.append(vectors)\n",
    "\n",
    "train_labels = np.zeros((len(questions_train),64),dtype=float)\n",
    "column_embeddings = []\n",
    "for idx,table in enumerate(tables_train):\n",
    "    cols = actual_col_train[idx]\n",
    "    table_column_tensor = []\n",
    "    x = 0\n",
    "    for j,col in enumerate(cols):\n",
    "        if col == label_cols_train[idx]:\n",
    "            x += 1\n",
    "            train_labels[idx][j] = 1.0\n",
    "        tokens = nltk.word_tokenize(unidecode(col).lower())\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                vectors.append(torch.tensor(model[token]))\n",
    "            except:\n",
    "                vectors.append(torch.zeros(100))\n",
    "        # sum\n",
    "        vectors = torch.sum(torch.stack(vectors, dim=0), dim = 0)\n",
    "        table_column_tensor.append(vectors)\n",
    "    assert(x == 1)\n",
    "    while len(table_column_tensor) < 64:\n",
    "        table_column_tensor.append(torch.zeros(100))\n",
    "    column_embeddings.append(torch.stack(table_column_tensor,dim = 0))\n",
    "train_labels = torch.Tensor(np.array(train_labels))\n",
    "\n",
    "val_labels = np.zeros((len(questions_test),64),dtype=float)\n",
    "val_column_embeddings = []\n",
    "for idx,table in enumerate(tables_test):\n",
    "    cols = actual_col_test[idx]\n",
    "    table_column_tensor = []\n",
    "    x = 0\n",
    "    for j,col in enumerate(cols):\n",
    "        if col == label_cols_test[idx]:\n",
    "            x += 1\n",
    "            val_labels[idx][j] = 1.0\n",
    "        tokens = nltk.word_tokenize(unidecode(col).lower())\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                vectors.append(torch.tensor(model[token]))\n",
    "            except:\n",
    "                vectors.append(torch.zeros(100))\n",
    "        # sum\n",
    "        vectors = torch.sum(torch.stack(vectors, dim=0), dim = 0)\n",
    "        table_column_tensor.append(vectors)\n",
    "    assert(x == 1)\n",
    "    while len(table_column_tensor) < 64:\n",
    "        table_column_tensor.append(torch.zeros(100))\n",
    "    val_column_embeddings.append(torch.stack(table_column_tensor,dim = 0))\n",
    "val_labels = torch.Tensor(np.array(val_labels))\n",
    "\n",
    "# Create the model\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "num_heads = 1\n",
    "dropout = 0.02\n",
    "classifier = Classifier(embedding_dim, hidden_dim, num_layers, num_heads, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.005)\n",
    "classifier.train()\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_list, columns_list, labels_list):\n",
    "        self.data = data_list\n",
    "        self.columns = columns_list\n",
    "        self.labels = labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'data': self.data[idx],\n",
    "            'columns': self.columns[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Build the dataloader\n",
    "dataset = CustomDataset(tokenized_questions, column_embeddings, train_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=5000, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_tokenized_questions, val_column_embeddings, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "print(\"Training start\")\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(200):\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    classifier.train()\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs = data['data'].to(device)\n",
    "        columns = data['columns'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(inputs, columns)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:\n",
    "            print(f'Epoch {epoch + 1}, batch {i + 1}: loss {running_loss / 5}')\n",
    "            print(f'Accuracy: {accuracy/(25 * 1000)}')\n",
    "            running_loss = 0.0\n",
    "            accuracy = 0\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        val_accuracy = 0\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            inputs = data['data'].to(device)\n",
    "            columns = data['columns'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            outputs = classifier(inputs, columns)\n",
    "            val_accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "        print(f'Validation accuracy: {val_accuracy/len(questions_test)}')\n",
    "        if val_accuracy/len(questions_test) > 0.9:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
