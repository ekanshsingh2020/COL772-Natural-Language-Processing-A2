{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 25000\n",
      "Number of tables: 25000\n",
      "Number of label columns: 25000\n",
      "Number of actual columns: 25000\n",
      "Number of questions: 5000\n",
      "Number of tables: 5000\n",
      "Number of label columns: 5000\n",
      "Number of actual columns: 5000\n",
      "Number of qids is  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekansh/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/ekansh/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "import gensim\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from unidecode import unidecode\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# from google.colab import drive\n",
    "# drive.mount('data')\n",
    "\n",
    "\n",
    "questions_train = []\n",
    "tables_train = []\n",
    "actual_col_train = []\n",
    "label_cols_train = []\n",
    "with open('data/A2_train.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Parse JSON data from each line\n",
    "        parsed_data = json.loads(line)\n",
    "        questions_train.append(parsed_data['question'])\n",
    "        tables_train.append(parsed_data['table'])\n",
    "        label_cols_train.append(parsed_data['label_col'][0])\n",
    "        actual_col_train.append(list(parsed_data['table']['cols']))\n",
    "\n",
    "# questions_train = clean_questions(questions_train)\n",
    "\n",
    "print('Number of questions:', len(questions_train))\n",
    "print('Number of tables:', len(tables_train))\n",
    "print('Number of label columns:', len(label_cols_train))\n",
    "print('Number of actual columns:', len(actual_col_train))\n",
    "\n",
    "questions_test = []\n",
    "tables_test = []\n",
    "actual_col_test = []\n",
    "label_cols_test = []\n",
    "qid_test = []\n",
    "with open('data/A2_val.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Parse JSON data from each line\n",
    "        parsed_data = json.loads(line)\n",
    "        questions_test.append(parsed_data['question'])\n",
    "        tables_test.append(parsed_data['table'])\n",
    "        label_cols_test.append(parsed_data['label_col'][0])\n",
    "        actual_col_test.append(list(parsed_data['table']['cols']))\n",
    "        qid_test.append(parsed_data['qid'])\n",
    "\n",
    "# questions_test = clean_questions(questions_test)\n",
    "\n",
    "print('Number of questions:', len(questions_test))\n",
    "print('Number of tables:', len(tables_test))\n",
    "print('Number of label columns:', len(label_cols_test))\n",
    "print('Number of actual columns:', len(actual_col_test))\n",
    "print('Number of qids is ', len(qid_test))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "embedding_dimension = 100\n",
    "hidden_dimension = 256\n",
    "num_layers = 2\n",
    "num_heads = 1\n",
    "dropout = 0.02\n",
    "\n",
    "max_len_question = 60\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pos_em = torch.zeros(max_len_question, embedding_dim)\n",
    "        division = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        position = torch.arange(0, max_len_question, dtype=torch.float).unsqueeze(1)\n",
    "        pos_em[:, 0::2] = torch.sin(position * division)\n",
    "        pos_em[:, 1::2] = torch.cos(position * division)\n",
    "        self.register_buffer('pos_em', pos_em)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.pos_em\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, num_heads, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout,batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        self.pos_embed = PositionalEmbedding(embedding_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, ques_vectors, column):\n",
    "        input_embedding = ques_vectors\n",
    "        contextual_embedding = self.transformer_encoder(input_embedding)\n",
    "        question_embedding  = torch.sum(contextual_embedding,dim = 1)\n",
    "        mat_mul = torch.nn.functional.normalize(column,dim = 2) * torch.nn.functional.normalize(question_embedding.unsqueeze(1), dim = 2)\n",
    "        dot_prod = torch.sum(mat_mul, dim=2)\n",
    "        return dot_prod\n",
    "\n",
    "\n",
    "def word2vec_questions(questions):\n",
    "    final_word2vec = []\n",
    "    pos_embed = PositionalEmbedding(100)\n",
    "    pos_em = pos_embed()\n",
    "    for i in range(len(questions)):\n",
    "        ques = questions[i]\n",
    "        ques = unidecode(ques)\n",
    "        ques_tokens = nltk.word_tokenize(ques.lower())\n",
    "        word2vec = []\n",
    "\n",
    "        for token in ques_tokens:\n",
    "            try:\n",
    "                word2vec.append(torch.tensor(model[token]))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        while len(word2vec) < max_len_question:\n",
    "            word2vec.append(torch.zeros(100))\n",
    "        \n",
    "        for j in range(len(word2vec)):\n",
    "            word2vec[j] = [x+y for x, y in zip(word2vec[j], pos_em[j])]\n",
    "        \n",
    "        word2vec = torch.stack(word2vec, dim=0)\n",
    "        final_word2vec.append(word2vec)\n",
    "    return final_word2vec\n",
    "\n",
    "def one_hot_label(actual_col, label_col):\n",
    "    one_hot = torch.zeros((len(actual_col), 64), dtype=float)\n",
    "    for i in range(len(actual_col)):\n",
    "        for j in range(len(actual_col[i])):\n",
    "            if actual_col[i][j] == label_col[i]:\n",
    "                one_hot[i][j] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def column_embed(actual_col):\n",
    "    final_embed = []\n",
    "    for i in range(len(actual_col)):\n",
    "        col = actual_col[i]\n",
    "        word_embed = []\n",
    "        for j in range(len(col)):\n",
    "            temp = unidecode(col[j])\n",
    "            tokens = nltk.word_tokenize(temp.lower())\n",
    "            within_word_embed = []\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    within_word_embed.append(torch.tensor(model[token]))\n",
    "                except:\n",
    "                    within_word_embed.append(torch.zeros(100))\n",
    "            within_word_embed = torch.sum(torch.stack(within_word_embed, dim=0), dim = 0)\n",
    "            word_embed.append(within_word_embed)\n",
    "        while len(word_embed) < 64:\n",
    "            word_embed.append(torch.zeros(100))\n",
    "        final_embed.append(torch.stack(word_embed, dim=0))\n",
    "    return final_embed\n",
    "\n",
    "\n",
    "questions_vectors_train = word2vec_questions(questions_train)\n",
    "questions_vectors_test = word2vec_questions(questions_test)\n",
    "\n",
    "one_hot_label_train = one_hot_label(actual_col_train, label_cols_train)\n",
    "one_hot_label_test = one_hot_label(actual_col_test, label_cols_test)\n",
    "\n",
    "col_embeddings_train = column_embed(actual_col_train)\n",
    "col_embeddings_test = column_embed(actual_col_test)\n",
    "\n",
    "classifier = Classifier(embedding_dimension, hidden_dimension, num_layers, num_heads, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.005)\n",
    "\n",
    "classifier.train()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_list, columns_list, labels_list):\n",
    "        self.data = data_list\n",
    "        self.columns = columns_list\n",
    "        self.labels = labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'data': self.data[idx],\n",
    "            'columns': self.columns[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "dataset = CustomDataset(questions_vectors_train, col_embeddings_train, one_hot_label_train)\n",
    "dataloader = DataLoader(dataset, batch_size=5000, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(questions_vectors_test, col_embeddings_test, one_hot_label_test)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "\n",
    "for epoch in range(500):\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    classifier.train()\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs = data['data'].to(device)\n",
    "        columns = data['columns'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(inputs, columns)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:\n",
    "            print(f'Epoch {epoch + 1}, batch {i + 1}: loss {running_loss / 5}')\n",
    "            print(f'Accuracy: {accuracy/(25 * 1000)}')\n",
    "            running_loss = 0.0\n",
    "            accuracy = 0\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        val_accuracy = 0\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            inputs = data['data'].to(device)\n",
    "            columns = data['columns'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            outputs = classifier(inputs, columns)\n",
    "            val_accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "        print(f'Validation accuracy: {val_accuracy/len(questions_test)}')\n",
    "        if val_accuracy/len(questions_test) > 0.9:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data = []\n",
    "for i in range(len(questions_vectors_train)):\n",
    "    temp_list = []\n",
    "    temp_list.append(questions_vectors_train[i])\n",
    "    temp_list.append(col_embeddings_train[i])\n",
    "    temp_list.append(one_hot_label_train[i])\n",
    "    batched_data.append(temp_list)\n",
    "\n",
    "val_batched_data = []\n",
    "for i in range(len(questions_vectors_test)):\n",
    "    temp_list = []\n",
    "    temp_list.append(questions_vectors_test[i])\n",
    "    temp_list.append(col_embeddings_test[i])\n",
    "    temp_list.append(one_hot_label_test[i])\n",
    "    val_batched_data.append(temp_list)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "\n",
    "for epoch in range(500):\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    random.shuffle(batched_data)\n",
    "    for i in range(0, len(batched_data), 5000):\n",
    "        batch = batched_data[i:i+5000]\n",
    "        inputs = []\n",
    "        columns = []\n",
    "        labels = []\n",
    "        for bat in batch:\n",
    "            inputs.append(bat[0])\n",
    "            columns.append(bat[1])\n",
    "            labels.append(bat[2])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(inputs, columns)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:\n",
    "            print(f'Epoch {epoch + 1}, batch {i + 1}: loss {running_loss / 5}')\n",
    "            print(f'Accuracy: {accuracy/(25 * 1000)}')\n",
    "            running_loss = 0.0\n",
    "            accuracy = 0\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        val_accuracy = 0\n",
    "        for i in range(0, len(val_batched_data), 1000):\n",
    "            batch = val_batched_data[i:i+1000]\n",
    "            inputs = []\n",
    "            columns = []\n",
    "            labels = []\n",
    "            for bat in batch:\n",
    "                inputs.append(bat[0])\n",
    "                columns.append(bat[1])\n",
    "                labels.append(bat[2])\n",
    "            outputs = classifier(inputs, columns)\n",
    "            val_accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "        print(f'Validation accuracy: {val_accuracy/len(questions_test)}')\n",
    "        if val_accuracy/len(questions_test) > 0.9:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
