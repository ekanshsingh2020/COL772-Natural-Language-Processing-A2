{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "weY1AQnNUqgm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim.downloader\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51ljxJFuX0aI",
        "outputId": "cdfb3293-5e52-437d-e8a6-cae46ee00d64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/ekansh/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTck2CH0Ie6h",
        "outputId": "05a4fdc8-65a6-4ed8-db54-fd47fa5967ee"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('Data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JlmkNN_Uqgp",
        "outputId": "754c0766-7981-4b3d-ffa5-d4e1fa57c3a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of questions: 25000\n",
            "Number of tables: 25000\n",
            "Number of label columns: 25000\n",
            "Number of actual columns: 25000\n",
            "Number of one-hot labels: 25000\n"
          ]
        }
      ],
      "source": [
        "questions = []\n",
        "tables = []\n",
        "actual_col = []\n",
        "label_cols = []\n",
        "one_hot_label = []\n",
        "\n",
        "# Read JSON data from file line by line\n",
        "with open('data/A2_train.jsonl', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        # Parse JSON data from each line\n",
        "        parsed_data = json.loads(line)\n",
        "\n",
        "        questions.append(parsed_data['question'])\n",
        "\n",
        "        tables.append(parsed_data['table'])\n",
        "        label_cols.append(parsed_data['label_col'])\n",
        "        # Extract actual column names from table\n",
        "        actual_col.append(list(parsed_data['table']['cols']))\n",
        "        # Extract ground truth index from label_col\n",
        "        one_hot_vector = []\n",
        "        for i in range(len(parsed_data['table']['cols'])):\n",
        "            if parsed_data['table']['cols'][i] in parsed_data['label_col']:\n",
        "                one_hot_vector.append(1.0)\n",
        "            else:\n",
        "                one_hot_vector.append(0.0)\n",
        "        # padd with zeros till 64\n",
        "        while len(one_hot_vector) < 64:\n",
        "            one_hot_vector.append(0.0)\n",
        "        one_hot_label.append(one_hot_vector)\n",
        "\n",
        "# make one_hot_label a tensor\n",
        "one_hot_label = torch.tensor(one_hot_label)\n",
        "\n",
        "print('Number of questions:', len(questions))\n",
        "print('Number of tables:', len(tables))\n",
        "print('Number of label columns:', len(label_cols))\n",
        "print('Number of actual columns:', len(actual_col))\n",
        "print('Number of one-hot labels:', len(one_hot_label))\n",
        "\n",
        "# word2vec_model = gensim.models.KeyedVectors.load('models/glove-wiki-gigaword-100')\n",
        "word2vec_model = gensim.downloader.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UNT42ydZUqgq"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding:\n",
        "    def __init__(self, hidden_dim, max_seq_len=60):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.positional_encoding = self.get_positional_encoding()\n",
        "\n",
        "    def get_positional_encoding(self):\n",
        "        pe = np.zeros((self.max_seq_len, self.hidden_dim))\n",
        "        position = np.arange(0, self.max_seq_len)[:, np.newaxis]\n",
        "        div_term = np.exp(np.arange(0, self.hidden_dim, 2) * (-np.log(10000.0) / self.hidden_dim))\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 1::2] = np.cos(position * div_term)\n",
        "        # convert to tensor\n",
        "        pe = torch.tensor(pe, dtype=torch.float32)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.positional_encoding[:seq_len, :]\n",
        "\n",
        "def get_word2vec_embeddings(sentence, word2vec_model):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    embeddings = []\n",
        "    # make an embeddings tensor\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            embeddings.append(word2vec_model[token])\n",
        "        except:\n",
        "            embeddings.append(np.zeros(100))\n",
        "    embeddings_array = np.stack(embeddings)\n",
        "    embeddings_array = torch.tensor(embeddings_array,dtype=torch.float32)\n",
        "    return embeddings_array\n",
        "\n",
        "def combine_embeddings(positional_embeddings, word_embeddings):\n",
        "    return positional_embeddings + word_embeddings\n",
        "\n",
        "def process_questions(questions, word2vec_model):\n",
        "    positional_encoder = PositionalEncoding(hidden_dim=100)\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Process each question\n",
        "    for question in questions:\n",
        "        # Get Word2Vec embeddings for each word\n",
        "        word_embeddings = get_word2vec_embeddings(question, word2vec_model)\n",
        "        # Get positional encodings for each word\n",
        "        seq_len = len(nltk.word_tokenize(question))\n",
        "        positional_embeddings = positional_encoder.forward(seq_len)\n",
        "        # print(positional_embeddings.shape)\n",
        "\n",
        "        # Combine positional and Word2Vec embeddings\n",
        "        combined_embeddings = combine_embeddings(positional_embeddings, word_embeddings)\n",
        "        combined_embeddings_tensor = torch.tensor(combined_embeddings,dtype=torch.float32)\n",
        "        all_embeddings.append(combined_embeddings_tensor)\n",
        "\n",
        "\n",
        "    # padd the embeddings to the same length\n",
        "    max_len = 60\n",
        "    for i in range(len(all_embeddings)):\n",
        "        if all_embeddings[i].shape[0] < max_len:\n",
        "            padding = torch.zeros(max_len - all_embeddings[i].shape[0], 100)\n",
        "            all_embeddings[i] = torch.cat((all_embeddings[i], padding), 0)\n",
        "        else:\n",
        "            all_embeddings[i] = all_embeddings[i][:max_len]\n",
        "\n",
        "    all_embeddings_tensor = torch.stack(all_embeddings)\n",
        "    # print(all_embeddings_tensor.shape)\n",
        "    return all_embeddings_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDNycLnHUqgq",
        "outputId": "b300ca47-cb42-4a55-cc44-b8f87c9aaed1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_7238/3696321757.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  actual_col_embeddings[i] = torch.tensor(actual_col_embeddings[i], dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([25000, 64, 100])\n"
          ]
        }
      ],
      "source": [
        "actual_col_embeddings = []\n",
        "padded_length = 64\n",
        "for col in actual_col:\n",
        "    temp_embedding = []\n",
        "    for word in col:\n",
        "        try:\n",
        "            temp_embedding.append(word2vec_model[word.lower()])\n",
        "        except:\n",
        "            temp_embedding.append(np.zeros(100))\n",
        "    # print(len(temp_embedding))\n",
        "    # padd the embeddings to the same length\n",
        "    if len(col) < padded_length:\n",
        "        padding = np.zeros(100)\n",
        "        for i in range(padded_length - len(col)):\n",
        "            temp_embedding.append(padding)\n",
        "    else:\n",
        "        temp_embedding = temp_embedding[:padded_length]\n",
        "    # print(len(temp_embedding))\n",
        "    actual_col_embeddings.append(temp_embedding)\n",
        "\n",
        "# convert the list to tensor\n",
        "    \n",
        "# for i in range(len(actual_col_embeddings)):\n",
        "#     actual_col_embeddings[i] = torch.tensor(actual_col_embeddings[i], dtype=torch.float32)\n",
        "# actual_col_embeddings = torch.stack(actual_col_embeddings)\n",
        "print(actual_col_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hfNaVd7UUqgr"
      },
      "outputs": [],
      "source": [
        "# class TransformerEncoder(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, num_layers, num_heads, dropout=0.1):\n",
        "#         super(TransformerEncoder, self).__init__()\n",
        "#         self.embedding_dim = input_dim\n",
        "#         self.transformer_encoder = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
        "#         self.transformer = nn.TransformerEncoder(self.transformer_encoder, num_layers)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         output = self.transformer(input)\n",
        "#         output = torch.tensor(output)\n",
        "#         return output\n",
        "\n",
        "# Example usage\n",
        "input_dim = 100  # Assuming each word embedding is of size 100\n",
        "hidden_dim = 256\n",
        "num_layers = 1\n",
        "num_heads = 1\n",
        "\n",
        "# Initialize transformer encoder\n",
        "# transformer_encoder = TransformerEncoder(input_dim, hidden_dim, num_layers, num_heads).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ccwsxx-HUqgr"
      },
      "outputs": [],
      "source": [
        "word_embeddings = process_questions(questions, word2vec_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "bfvVtj_UUqgr"
      },
      "outputs": [],
      "source": [
        "class TextClassifier (nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, num_classes):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding_dim = input_dim\n",
        "        self.transformer_encoder = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=0.1)\n",
        "        self.transformer = nn.TransformerEncoder(self.transformer_encoder, num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, word_embedding, col_embeding):\n",
        "        # Compute dot product between contextual embeddings and one_hot_label\n",
        "        contextual_embeddings = self.transformer(word_embedding)\n",
        "        question_vectors = torch.sum(contextual_embeddings, dim=1)\n",
        "\n",
        "        # print(question_vectors.shape)\n",
        "        question_vectors.unsqueeze_(1)\n",
        "        # print(question_vectors.shape)            \n",
        "            \n",
        "        col_embeding = torch.tensor(col_embeding, dtype=torch.float32)\n",
        "        # print(col_embeding.shape)\n",
        "        # print(\"===============================\")\n",
        "\n",
        "        temp = question_vectors * col_embeding\n",
        "        dot_products = torch.sum(temp, dim=-1)\n",
        "\n",
        "        # print(dot_products.shape)\n",
        "\n",
        "\n",
        "\n",
        "        # for i in range(len(question_vectors)):\n",
        "        #     dot_products.append(torch.matmul(question_vectors[i], col_embeding[i].t()))\n",
        "        # dot_products = torch.stack(dot_products)\n",
        "        # print(dot_products.shape)\n",
        "\n",
        "        # Compute softmax along the last dimension (column-wise softmax)\n",
        "        # softmax_output = torch.tensor(F.softmax(dot_products, dim=-1))\n",
        "        softmax_output = F.softmax(dot_products, dim=-1)\n",
        "        # print(softmax_output.shape)\n",
        "        # print(softmax_output[0])\n",
        "        return softmax_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xxxi7PXZUqgs"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l8w4GbgLUqgr"
      },
      "outputs": [],
      "source": [
        "weights = [1.0, 1.0/2, 1.0/3, 1.0/4, 1.0/5, 1.0/6, 1.0/7, 1.0/8, 1.0/9, 1.0/10, 1.0/11, 1.0/12, 1.0/13, 1.0/14, 1.0/15, 1.0/16, 1.0/17, 1.0/18, 1.0/19, 1.0/20, 1.0/21, 1.0/22, 1.0/23, 1.0/24, 1.0/25, 1.0/26, 1.0/27, 1.0/28, 1.0/29, 1.0/30, 1.0/31, 1.0/32, 1.0/33, 1.0/34, 1.0/35, 1.0/36, 1.0/37, 1.0/38, 1.0/39, 1.0/40, 1.0/41, 1.0/42, 1.0/43, 1.0/44, 1.0/45, 1.0/46, 1.0/47, 1.0/48, 1.0/49, 1.0/50, 1.0/51, 1.0/52, 1.0/53, 1.0/54, 1.0/55, 1.0/56, 1.0/57, 1.0/58, 1.0/59, 1.0/60, 1.0/61, 1.0/62, 1.0/63, 1.0/64]\n",
        "\n",
        "# change the weights to tensor\n",
        "weights = torch.tensor(weights, dtype=torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRS5BZO2Uqgs",
        "outputId": "eceb1ac5-47ce-4bcd-f458-839be865847e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ekansh/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "classifier = TextClassifier(input_dim, hidden_dim, num_layers, num_heads, 64).to(device)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0u-vIIpQUqgs"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_list, columns_list, labels_list):\n",
        "        self.data = data_list\n",
        "        self.columns = columns_list\n",
        "        self.labels = labels_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'data': self.data[idx],\n",
        "            'columns': self.columns[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Build the dataloader\n",
        "dataset = CustomDataset(word_embeddings, actual_col_embeddings, one_hot_label)\n",
        "dataloader = DataLoader(dataset, batch_size=5000, shuffle=True)\n",
        "\n",
        "# val_dataset = CustomDataset(val_tokenized_questions, val_column_embeddings, val_labels)\n",
        "# val_dataloader = DataLoader(val_dataset, batch_size=1000, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbTmoc2dUqgs",
        "outputId": "55fa1d4d-640a-4102-d81a-98a82ee3728c"
      },
      "outputs": [],
      "source": [
        "print(\"Training start\")\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1000):\n",
        "    running_loss = 0.0\n",
        "    accuracy = 0\n",
        "    classifier.train()\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs = data['data'].to(device)\n",
        "        columns = data['columns'].to(device)\n",
        "        labels = data['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classifier(inputs, columns)\n",
        "        loss = criterion(outputs, labels)\n",
        "        accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 5 == 4:\n",
        "            print(f'Epoch {epoch + 1}, batch {i + 1}: loss {running_loss / 5}')\n",
        "            print(f'Accuracy: {accuracy/(25 * 1000)}')\n",
        "            running_loss = 0.0\n",
        "            accuracy = 0\n",
        "    classifier.eval()\n",
        "    # with torch.no_grad():\n",
        "    #     val_accuracy = 0\n",
        "    #     for i, data in enumerate(val_dataloader, 0):\n",
        "    #         inputs = data['data'].to(device)\n",
        "    #         columns = data['columns'].to(device)\n",
        "    #         labels = data['labels'].to(device)\n",
        "    #         outputs = classifier(inputs, columns)\n",
        "    #         val_accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
        "    #     print(f'Validation accuracy: {val_accuracy/len(val_questions)}')\n",
        "    #     if val_accuracy/len(val_questions) > 0.9:\n",
        "    #         break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
