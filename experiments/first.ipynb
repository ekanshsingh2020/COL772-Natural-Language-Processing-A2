{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "tables = []\n",
    "actual_col = []\n",
    "label_cols = []\n",
    "one_hot_label = []\n",
    "\n",
    "# Read JSON data from file line by line\n",
    "with open('data/A2_train.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Parse JSON data from each line\n",
    "        parsed_data = json.loads(line)\n",
    "        \n",
    "        questions.append(parsed_data['question'])\n",
    "        \n",
    "        tables.append(parsed_data['table'])\n",
    "        label_cols.append(parsed_data['label_col'])\n",
    "        # Extract actual column names from table\n",
    "        actual_col.append(list(parsed_data['table']['cols']))\n",
    "        # Extract ground truth index from label_col\n",
    "        one_hot_vector = []\n",
    "        for i in range(len(parsed_data['table']['cols'])):\n",
    "            if i in parsed_data['label_col']:\n",
    "                one_hot_vector.append(1.0)\n",
    "            else:\n",
    "                one_hot_vector.append(0.0)\n",
    "        # padd with zeros till 64\n",
    "        while len(one_hot_vector) < 64:\n",
    "            one_hot_vector.append(0.0)\n",
    "        one_hot_label.append(one_hot_vector)\n",
    "\n",
    "# make one_hot_label a tensor\n",
    "one_hot_label = torch.tensor(one_hot_label)\n",
    "\n",
    "print('Number of questions:', len(questions))\n",
    "print('Number of tables:', len(tables))\n",
    "print('Number of label columns:', len(label_cols))\n",
    "print('Number of actual columns:', len(actual_col))\n",
    "print('Number of one-hot labels:', len(one_hot_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load('models/glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, hidden_dim, max_seq_len=60):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.positional_encoding = self.get_positional_encoding()\n",
    "\n",
    "    def get_positional_encoding(self):\n",
    "        pe = np.zeros((self.max_seq_len, self.hidden_dim))\n",
    "        position = np.arange(0, self.max_seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, self.hidden_dim, 2) * (-np.log(10000.0) / self.hidden_dim))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        return self.positional_encoding[:seq_len, :]\n",
    "    \n",
    "def get_word2vec_embeddings(sentence, word2vec_model):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            embeddings.append(word2vec_model[token])\n",
    "        except:\n",
    "            embeddings.append(np.zeros(100))\n",
    "    embeddings_array = np.stack(embeddings)\n",
    "    return embeddings_array\n",
    "\n",
    "def combine_embeddings(positional_embeddings, word_embeddings):\n",
    "    return positional_embeddings + word_embeddings\n",
    "\n",
    "def process_questions(questions, word2vec_model):\n",
    "    # Initialize PositionalEncoding\n",
    "    positional_encoder = PositionalEncoding(hidden_dim=100)\n",
    "\n",
    "    # Initialize empty numpy array to store embeddings for each word in each question\n",
    "    all_embeddings = []\n",
    "\n",
    "    # Process each question\n",
    "    for question in questions:\n",
    "        # Get Word2Vec embeddings for each word\n",
    "        word_embeddings = get_word2vec_embeddings(question, word2vec_model)\n",
    "        # Get positional encodings for each word\n",
    "        seq_len = len(nltk.word_tokenize(question))\n",
    "        positional_embeddings = positional_encoder.forward(seq_len)\n",
    "        # print(positional_embeddings.shape)\n",
    "\n",
    "        # Combine positional and Word2Vec embeddings\n",
    "        combined_embeddings = combine_embeddings(positional_embeddings, word_embeddings)\n",
    "        # print(combined_embeddings.shape)\n",
    "        combined_embeddings_tensor = torch.tensor(combined_embeddings, dtype=torch.float32)\n",
    "        # print(combined_embeddings_tensor.shape)\n",
    "        # Append embeddings for each word to the list\n",
    "        all_embeddings.append(combined_embeddings_tensor)\n",
    "    \n",
    "\n",
    "    # padd the embeddings to the same length\n",
    "    max_len = 60\n",
    "    for i in range(len(all_embeddings)):\n",
    "        if all_embeddings[i].shape[0] < max_len:\n",
    "            padding = torch.zeros(max_len - all_embeddings[i].shape[0], 100)\n",
    "            all_embeddings[i] = torch.cat((all_embeddings[i], padding), 0)\n",
    "        else:\n",
    "            all_embeddings[i] = all_embeddings[i][:max_len]\n",
    "\n",
    "    all_embeddings_tensor = torch.stack(all_embeddings)\n",
    "    print(all_embeddings_tensor.shape)\n",
    "    return all_embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding_dim = input_dim\n",
    "        self.transformer_encoder = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder, num_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.transformer(input)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "input_dim = 100  # Assuming each word embedding is of size 100\n",
    "hidden_dim = 100\n",
    "num_layers = 2\n",
    "num_heads = 2\n",
    "\n",
    "# Initialize transformer encoder\n",
    "transformer_encoder = TransformerEncoder(input_dim, hidden_dim, num_layers, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = process_questions(questions, word2vec_model)\n",
    "contextual_embeddings = transformer_encoder.forward(word_embeddings)\n",
    "print(\"Contextual embeddings shape:\", contextual_embeddings.shape)\n",
    "\n",
    "question_embeddings = torch.sum(contextual_embeddings, dim=1)\n",
    "print(\"Question embeddings shape:\", question_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier (nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.transformer_encoder = TransformerEncoder(input_dim, hidden_dim, num_layers, num_heads)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)   \n",
    "\n",
    "    def forward(self, question_vectors, col_embeding):\n",
    "        # Compute dot product between contextual embeddings and one_hot_label\n",
    "        dot_products = []\n",
    "        for i in range(len(question_vectors)):\n",
    "            dot_products.append(torch.matmul(question_vectors[i], col_embeding[i].t()))\n",
    "        dot_products = torch.stack(dot_products)\n",
    "\n",
    "        # Compute softmax along the last dimension (column-wise softmax)\n",
    "        softmax_output = F.softmax(dot_products, dim=-1)\n",
    "\n",
    "        return softmax_output\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a tensor of tensors\n",
    "actual_col_embeddings = []\n",
    "padded_length = 64\n",
    "for col in actual_col:\n",
    "    temp_embedding = []\n",
    "    for word in col:\n",
    "        try:\n",
    "            temp_embedding.append(word2vec_model[word.lower()])\n",
    "        except:\n",
    "            temp_embedding.append(np.zeros(100))\n",
    "    # print(len(temp_embedding))\n",
    "    # padd the embeddings to the same length\n",
    "    if len(col) < padded_length:\n",
    "        padding = np.zeros(100)\n",
    "        for i in range(padded_length - len(col)):\n",
    "            temp_embedding.append(padding)\n",
    "    else:\n",
    "        temp_embedding = temp_embedding[:padded_length]\n",
    "    # print(len(temp_embedding))\n",
    "    actual_col_embeddings.append(temp_embedding)\n",
    "\n",
    "# convert the list to tensor\n",
    "for i in range(len(actual_col_embeddings)):\n",
    "    actual_col_embeddings[i] = torch.tensor(actual_col_embeddings[i], dtype=torch.float32)\n",
    "actual_col_embeddings = torch.stack(actual_col_embeddings)\n",
    "print(actual_col_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1.0, 1.0/2, 1.0/3, 1.0/4, 1.0/5, 1.0/6, 1.0/7, 1.0/8, 1.0/9, 1.0/10, 1.0/11, 1.0/12, 1.0/13, 1.0/14, 1.0/15, 1.0/16, 1.0/17, 1.0/18, 1.0/19, 1.0/20, 1.0/21, 1.0/22, 1.0/23, 1.0/24, 1.0/25, 1.0/26, 1.0/27, 1.0/28, 1.0/29, 1.0/30, 1.0/31, 1.0/32, 1.0/33, 1.0/34, 1.0/35, 1.0/36, 1.0/37, 1.0/38, 1.0/39, 1.0/40, 1.0/41, 1.0/42, 1.0/43, 1.0/44, 1.0/45, 1.0/46, 1.0/47, 1.0/48, 1.0/49, 1.0/50, 1.0/51, 1.0/52, 1.0/53, 1.0/54, 1.0/55, 1.0/56, 1.0/57, 1.0/58, 1.0/59, 1.0/60, 1.0/61, 1.0/62, 1.0/63, 1.0/64]\n",
    "\n",
    "# change the weights to tensor\n",
    "weights = torch.tensor(weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TextClassifier(input_dim, hidden_dim, num_layers, num_heads, len(actual_col))\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training start\")\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    classifier.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = classifier(question_embeddings, actual_col_embeddings)\n",
    "    loss = criterion(outputs, one_hot_label)\n",
    "    accuracy += (outputs.argmax(dim=1) == one_hot_label.argmax(dim=1)).sum().item()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    if i % 5 == 4:\n",
    "        # print(f'Epoch {epoch + 1}: loss {running_loss}')\n",
    "        print(f'Accuracy: {accuracy / len(question_embeddings)}')\n",
    "        running_loss = 0.0\n",
    "        accuracy = 0\n",
    "    classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def _init_(self, data_list, columns_list, labels_list):\n",
    "#         self.data = data_list\n",
    "#         self.columns = columns_list\n",
    "#         self.labels = labels_list\n",
    "\n",
    "#     def _len_(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def _getitem_(self, idx):\n",
    "#         sample = {\n",
    "#             'data': self.data[idx],\n",
    "#             'columns': self.columns[idx],\n",
    "#             'labels': self.labels[idx]\n",
    "#         }\n",
    "#         return sample\n",
    "\n",
    "# # Build the dataloader\n",
    "# dataset = CustomDataset(tokenized_questions, column_embeddings, train_labels)\n",
    "# dataloader = DataLoader(dataset, batch_size=5000, shuffle=True)\n",
    "\n",
    "# val_dataset = CustomDataset(val_tokenized_questions, val_column_embeddings, val_labels)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "# print(\"Training start\")\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(1000):\n",
    "#     running_loss = 0.0\n",
    "#     accuracy = 0\n",
    "#     classifier.train()\n",
    "#     for i, data in enumerate(dataloader, 0):\n",
    "#         inputs = data['data'].to(device)\n",
    "#         columns = data['columns'].to(device)\n",
    "#         labels = data['labels'].to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = classifier(inputs, columns)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 5 == 4:\n",
    "#             print(f'Epoch {epoch + 1}, batch {i + 1}: loss {running_loss / 5}')\n",
    "#             print(f'Accuracy: {accuracy/(25 * 1000)}')\n",
    "#             running_loss = 0.0\n",
    "#             accuracy = 0\n",
    "#     classifier.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_accuracy = 0\n",
    "#         for i, data in enumerate(val_dataloader, 0):\n",
    "#             inputs = data['data'].to(device)\n",
    "#             columns = data['columns'].to(device)\n",
    "#             labels = data['labels'].to(device)\n",
    "#             outputs = classifier(inputs, columns)\n",
    "#             val_accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "#         print(f'Validation accuracy: {val_accuracy/len(val_questions)}')\n",
    "#         if val_accuracy/len(val_questions) > 0.9:\n",
    "#             break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
