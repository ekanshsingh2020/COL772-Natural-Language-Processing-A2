{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wARnnMo1oIPd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim.downloader\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import re\n",
        "from unidecode import unidecode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwsQxAesoJAj",
        "outputId": "729239df-830b-4170-d265-76cdef6ee1a9"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('data')\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_questions(questions):\n",
        "    cleaned_questions = []\n",
        "    for question in questions:\n",
        "        question = question.lower()\n",
        "        question = unidecode(question)\n",
        "        cleaned_question = re.sub(r\"[^\\w\\s]\", \"\", question)  # Remove punctuation\n",
        "        cleaned_questions.append(cleaned_question)\n",
        "    return cleaned_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "084cbP_boIPf",
        "outputId": "bc0f1979-188a-4b7e-9e8d-891759d4aa60"
      },
      "outputs": [],
      "source": [
        "questions_train = []\n",
        "tables_train = []\n",
        "actual_col_train = []\n",
        "label_cols_train = []\n",
        "# Read JSON data from file line by line\n",
        "with open('data/A2_train.jsonl', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        # Parse JSON data from each line\n",
        "        parsed_data = json.loads(line)\n",
        "        questions_train.append(parsed_data['question'])\n",
        "        tables_train.append(parsed_data['table'])\n",
        "        label_cols_train.append(parsed_data['label_col'][0])\n",
        "        actual_col_train.append(list(parsed_data['table']['cols']))\n",
        "\n",
        "questions_train = clean_questions(questions_train)\n",
        "\n",
        "print('Number of questions:', len(questions_train))\n",
        "print('Number of tables:', len(tables_train))\n",
        "print('Number of label columns:', len(label_cols_train))\n",
        "print('Number of actual columns:', len(actual_col_train))\n",
        "\n",
        "# word2vec_model = gensim.models.KeyedVectors.load('models/glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t29POyc_oIPh",
        "outputId": "b9f19d78-940d-475b-deb2-86846918f7a7"
      },
      "outputs": [],
      "source": [
        "questions_test = []\n",
        "tables_test = []\n",
        "actual_col_test = []\n",
        "label_cols_test = []\n",
        "qid_test = []\n",
        "# Read JSON data from file line by line\n",
        "with open('data/A2_val.jsonl', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        # Parse JSON data from each line\n",
        "        parsed_data = json.loads(line)\n",
        "        questions_test.append(parsed_data['question'])\n",
        "        tables_test.append(parsed_data['table'])\n",
        "        label_cols_test.append(parsed_data['label_col'][0])\n",
        "        actual_col_test.append(list(parsed_data['table']['cols']))\n",
        "        qid_test.append(parsed_data['qid'])\n",
        "\n",
        "questions_test = clean_questions(questions_test)\n",
        "\n",
        "print('Number of questions:', len(questions_test))\n",
        "print('Number of tables:', len(tables_test))\n",
        "print('Number of label columns:', len(label_cols_test))\n",
        "print('Number of actual columns:', len(actual_col_test))\n",
        "print('Number of qids is ', len(qid_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNHS4f4ioIPi"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len=60):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embedding_dim)\n",
        "        self.max_len = max_len\n",
        "        self.embedding_dim = embedding_dim\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.pe\n",
        "\n",
        "def combine_embeddings(positional_embeddings, word_embeddings):\n",
        "    return positional_embeddings + word_embeddings\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, num_heads, dropout, max_len=60):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.pos_encoding = PositionalEncoding(embedding_dim, max_len)\n",
        "        self.encode = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout,batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(self.encode, num_layers)\n",
        "\n",
        "\n",
        "    def forward(self, text_vectors, column):\n",
        "        position = self.pos_encoding()\n",
        "        final_embed = position + text_vectors\n",
        "        contextual_embed = self.encoder(final_embed)\n",
        "        question_embed  = torch.sum(contextual_embed,dim = 1)\n",
        "        mat_mul = torch.nn.functional.normalize(column,dim = 2) * torch.nn.functional.normalize(question_embed.unsqueeze(1), dim = 2)\n",
        "        dot_product = torch.sum(mat_mul, dim=2)\n",
        "        return dot_product\n",
        "\n",
        "model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "\n",
        "def get_one_hot_vectors(sentences, actual_cols, label_cols):\n",
        "    one_hot_vectors = torch.zeros((len(sentences), 64),dtype=float)\n",
        "    for i in range(len(sentences)):\n",
        "        cols = actual_cols[i]\n",
        "        for j in range(len(cols)):\n",
        "            if cols[j]==label_cols[i]:\n",
        "                one_hot_vectors[i][j] = 1.0\n",
        "    return one_hot_vectors\n",
        "\n",
        "\n",
        "def column_embeddings(tables,actual_col):\n",
        "    column_embeddings = []\n",
        "    for i in range(len(tables)):\n",
        "        cols = actual_col[i]\n",
        "        col_tensor = []\n",
        "        for j in range(len(cols)):\n",
        "            tokens = nltk.word_tokenize(cols[j].lower())\n",
        "            vectors = []\n",
        "            for token in tokens:\n",
        "                try:\n",
        "                    vectors.append(torch.tensor(model[token]))\n",
        "                except:\n",
        "                    vectors.append(torch.zeros(100))\n",
        "            vectors = torch.sum(torch.stack(vectors,dim=0),dim=0)\n",
        "            col_tensor.append(vectors)\n",
        "        while len(col_tensor) < 64:\n",
        "            col_tensor.append(torch.zeros(100))\n",
        "        column_embeddings.append(torch.stack(col_tensor,dim=0))\n",
        "    return column_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag6J9oafoIPj"
      },
      "outputs": [],
      "source": [
        "train_labels = get_one_hot_vectors(questions_train, actual_col_train, label_cols_train)\n",
        "column_embeddings_train = column_embeddings(tables_train, actual_col_train)\n",
        "test_labels = get_one_hot_vectors(questions_test, actual_col_test, label_cols_test)\n",
        "column_embeddings_test = column_embeddings(tables_test, actual_col_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APsluPXxoIPk"
      },
      "outputs": [],
      "source": [
        "question_vectors_train = []\n",
        "for question in questions_train:\n",
        "    tokens = nltk.word_tokenize(question)\n",
        "\n",
        "    # Convert the tokens to word vectors\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            vectors.append(torch.tensor(model[token]))\n",
        "        except:\n",
        "            pass\n",
        "    # pad to 100 tokens\n",
        "    while len(vectors) < 60:\n",
        "        vectors.append(torch.zeros(100))\n",
        "    # concatenate the vectors to one tensor\n",
        "    vectors = torch.stack(vectors, dim=0)\n",
        "    question_vectors_train.append(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAHbYjPooIPl"
      },
      "outputs": [],
      "source": [
        "question_vectors_test = []\n",
        "for question in questions_test:\n",
        "    tokens = nltk.word_tokenize(question)\n",
        "\n",
        "    # Convert the tokens to word vectors\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            vectors.append(torch.tensor(model[token]))\n",
        "        except:\n",
        "            pass\n",
        "    # pad to 100 tokens\n",
        "    while len(vectors) < 60:\n",
        "        vectors.append(torch.zeros(100))\n",
        "    # concatenate the vectors to one tensor\n",
        "    vectors = torch.stack(vectors, dim=0)\n",
        "    question_vectors_test.append(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp9F3HKAoIPl"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 64\n",
        "num_layers = 2\n",
        "num_heads = 1\n",
        "dropout = 0.02\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaYulap7oIPn",
        "outputId": "00087877-fc0e-40b4-fe68-6500cd9aa6cd"
      },
      "outputs": [],
      "source": [
        "classifier = Classifier(embedding_dim, hidden_dim, num_layers, num_heads, dropout).to(device)\n",
        "class_weights = torch.tensor([1.0, 1.0/2, 1.0/3, 1.0/4, 1.0/5, 1.0/6, 1.0/7, 1.0/8, 1.0/9, 1.0/10, 1.0/11, 1.0/12, 1.0/13, 1.0/14, 1.0/15, 1.0/16, 1.0/17, 1.0/18, 1.0/19, 1.0/20, 1.0/21, 1.0/22, 1.0/23, 1.0/24, 1.0/25, 1.0/26, 1.0/27, 1.0/28, 1.0/29, 1.0/30, 1.0/31, 1.0/32, 1.0/33, 1.0/34, 1.0/35, 1.0/36, 1.0/37, 1.0/38, 1.0/39, 1.0/40, 1.0/41, 1.0/42, 1.0/43, 1.0/44, 1.0/45, 1.0/46, 1.0/47, 1.0/48, 1.0/49, 1.0/50, 1.0/51, 1.0/52, 1.0/53, 1.0/54, 1.0/55, 1.0/56, 1.0/57, 1.0/58, 1.0/59, 1.0/60, 1.0/61, 1.0/62, 1.0/63, 1.0/64]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.005)\n",
        "classifier.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZTrMJoYoIPn"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_list, columns_list, labels_list):\n",
        "        self.data = data_list\n",
        "        self.columns = columns_list\n",
        "        self.labels = labels_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'data': self.data[idx],\n",
        "            'columns': self.columns[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Build the dataloader\n",
        "dataset_train = CustomDataset(question_vectors_train, column_embeddings_train, train_labels)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=5000, shuffle=True)\n",
        "\n",
        "dataset_test = CustomDataset(question_vectors_test, column_embeddings_test, test_labels)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=1000, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23THb0n4oIPn",
        "outputId": "3d5ef1d7-e82e-4ef7-83ca-ed177fcfa9b1"
      },
      "outputs": [],
      "source": [
        "print(\"Training start\")\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1000):\n",
        "    running_loss = 0.0\n",
        "    accuracy = 0\n",
        "    classifier.train()\n",
        "    for i, data in enumerate(dataloader_train, 0):\n",
        "        inputs = data['data'].to(device)\n",
        "        columns = data['columns'].to(device)\n",
        "        labels = data['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classifier(inputs, columns)\n",
        "        loss = criterion(outputs, labels)\n",
        "        accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 5 == 4:\n",
        "            print(f'Epoch {epoch + 1}, batch {i + 1}: loss {running_loss / 5}')\n",
        "            print(f'Accuracy: {accuracy/(25 * 1000)}')\n",
        "            running_loss = 0.0\n",
        "            accuracy = 0\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        val_accuracy = 0\n",
        "        for i, data in enumerate(dataloader_test, 0):\n",
        "            inputs = data['data'].to(device)\n",
        "            columns = data['columns'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "            outputs = classifier(inputs, columns)\n",
        "            val_accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
        "        print(f'Validation accuracy: {val_accuracy/len(questions_test)}')\n",
        "        if val_accuracy/len(questions_test) > 0.9:\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
